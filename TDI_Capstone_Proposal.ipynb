{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TDI Capstone Proposal\n######                        by Saani Rawat\n######                        02/23/2020"},{"metadata":{},"cell_type":"markdown","source":"Job markets, in today's world, are extremely competitive. When we talk about data science market, the level of competition is fierce. Employees want to identify the open positions that best suit their long-term goals and needs. On the other hand, employers wish to attract best talent out of the available pool of candidates and stay ahead of their competition.\n\nThe purpose of the project is to analyse the current market condition of data-related jobs in major metropolitan cities: New York City, San Francisco, Los Angeles, Charlotte, Boston and Washington. I used web scraping tools in Python to extrapolate job postings data. Data were obtained using web scraping techniques (using Python) on Indeed's website.\nIn this notebook, we use the data scraped using \"Job Postings data scraping.py\" to identify interest patterns and relationships in the extracted dataset.\n"},{"metadata":{},"cell_type":"markdown","source":"## <font color =green> 1. Data Source </font>"},{"metadata":{},"cell_type":"markdown","source":"Data for major U.S metropolitan cities was scraped from Indeed's website. source: https://www.indeed.com\n\nTo understand how the data was scraped, see <font color=yellow> Job Postings data scraping.py </font>"},{"metadata":{},"cell_type":"markdown","source":"## <font color =green> 2. Data Description </font>"},{"metadata":{},"cell_type":"markdown","source":"Dataset name: data_science_jobs_df.csv\n\n**Columns** -\n\n*Unnamed column - contains index*\n\n*0 - Job Title*\n\n*1 - Company's Name*\n\n*2 - Company's Location*\n\n*3 - Salary*\n\n*4 - Job Summary*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd \nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color =green> 3. Data Cleaning </font>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Scraped data is often times quite messy. Hence, appropriate cleaning needs to be performed on datasets to make them comprehensible and suitable for analysis.\nIn this section, we identify and eliminate missing values, duplicates and other non-sensical observations."},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Reading in the dataset\ndata_science_jobs_df = pd.read_csv(\"../input/data_science_jobs_df.csv\")\ndf = pd.DataFrame(data_science_jobs_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the first 5 rows\ndata_science_jobs_df.head()\n# The column names are not very intuitive. Let's change them.\ncols = {'Unnamed: 0' : 'ID', '0': 'Title', '1' : 'Company', '2' : 'Company_Address', '3' : 'Salary', '4': 'Summary'}\ndf = df.rename(columns = cols)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the n.o of obs in the dataset\nlen(df)\ndf.loc[0:100,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A lot of the observations for the Salary column in the dataset have missing values. For our purposes, salary is the one of the most important columns. Hence, *removing job postings which do not reveal the offered salary*."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing observations with no salary information\ndf_sal = df.loc[~df.loc[:, 'Salary'].isnull(),:]\n# Checking length of the new dataset\nlen(df_sal)\n# nobs removed\nlen(df) - len(df_sal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"26012 observations did not contain salary information and hence, were removed from the dataset."},{"metadata":{},"cell_type":"markdown","source":"Data scraped from web pages often contains duplicates because the web pages often redirect to the same page during iteration. Hence, checking if any duplicate observations exist in our dataset with populated salary information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the n.o of duplicates in the dataset based on Title, Company, Company_Address and Salary\ndf_sal[df_sal.duplicated(['Title', 'Company', 'Company_Address', 'Salary'])]\n# len(df_sal[df_sal.duplicated(['Title', 'Company', 'Company_Address', 'Salary'])])\n# 3736\n# An e.g of duplicate observation -\ndf_sal.loc[(df_sal['Company'] == 'Qloo') & (df_sal['Title'] == 'Senior Data Scientist') , :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data contains** a lot **of duplicates. We need to get rid of these as they can skew our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing duplicates\ndf_sal_no_dup = df_sal.drop_duplicates(subset = ['Title', 'Company', 'Company_Address', 'Salary'])\nlen(df_sal_no_dup)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are left with 964 valid (dare I say clean) observations for our analysis."},{"metadata":{},"cell_type":"markdown","source":"Next, we need to clean the columns so that appropriate analysis can be performed.\n1. We need to extract city out of company address \n2. Salary range is given instead of salary, and that too in different formats (monthly, yearly, daily etc.). We need to align them.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting city out of company address\npd.options.mode.chained_assignment = None  # default='warn'\ndf_sal_no_dup.loc[:, 'City'] = df_sal_no_dup['Company_Address'].apply(lambda x: str(x).split(',')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting year, month or day out of salary\ndf_sal_no_dup.loc[:, 'Sal_type'] = df_sal_no_dup['Salary'].apply(lambda x: str(x).split(' ')[-1])\n\n# Extracting max of the salary range\ndf_sal_no_dup.loc[:, 'Sal_max'] = df_sal_no_dup['Salary'].apply(lambda x: str(x).split(' ')[-3])\n# Removing dollar sign and converting column into string\ndf_sal_no_dup.loc[:, 'Sal_max'] = df_sal_no_dup['Salary'].apply(lambda x: str(x).split('$')[1])\ndf_sal_no_dup.loc[:, 'Sal_max'] = df_sal_no_dup['Sal_max'].apply(lambda x: str(x).split(' ')[0])\n\n# Converting all monthly, hourly, and weekly salaries into yearly\n## Yearly\nmon_bool = df_sal_no_dup['Sal_type'] == 'year'\ndf_sal_no_dup.loc[(mon_bool), 'Annual_Max_Salary'] = df_sal_no_dup['Sal_max'].apply(lambda x: float(str(x).replace(',','')))\n## Monthly\nmon_bool = df_sal_no_dup['Sal_type'] == 'month'\ndf_sal_no_dup.loc[(mon_bool), 'Annual_Max_Salary'] = df_sal_no_dup['Sal_max'].apply(lambda x: float(str(x).replace(',','')) * 12)\n## Hourly (assuming 40 work hours and 52 weeks)\nhour_bool = df_sal_no_dup['Sal_type'] == 'hour'\ndf_sal_no_dup.loc[(hour_bool), 'Annual_Max_Salary'] = df_sal_no_dup['Sal_max'].apply(lambda x: float(str(x).replace(',','')) * 40 * 52)\n## Weekly\nweek_bool = df_sal_no_dup['Sal_type'] == 'week'\ndf_sal_no_dup.loc[(week_bool), 'Annual_Max_Salary'] = df_sal_no_dup['Sal_max'].apply(lambda x: float(str(x).replace(',','')) * 52)\n## Removing class Salary type and resetting index\ndf_sal_no_dup = df_sal_no_dup.loc[~(df_sal_no_dup['Sal_type'] == 'class'), :].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing observations with no cities\ndf_sal_no_dup = df_sal_no_dup[~(df_sal_no_dup['City'] == 'nan')]\n# nobs = 961 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, our data is ready for analysis. In the next section, we will explore this data and use visualization to recognize patterns.\n    \n"},{"metadata":{},"cell_type":"markdown","source":"## <font color =green> 4. Exploratory Data Analysis </font>"},{"metadata":{},"cell_type":"markdown","source":"In this section, we use data visualization techniques to answer interesting questions.\n\nSpecifically, we want to know the following -\n\n1. *how many data science jobs are available per city?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a box plot with n.o of data science job openings in different locations\n# plot1 = df_sal_no_dup[['ID','City']].groupby(['City']).agg('count').plot(kind = 'bar', legend = False, title = \"Number of Data Science Postings by City\")\nplot1 = df_sal_no_dup[['ID','City']].groupby(['City']).agg('count').reset_index().rename(columns = {'ID' : 'Count'})\nsns.set_style(\"dark\")\nsns.barplot(x = plot1['City'], y = plot1['Count'])\nplt.xticks(rotation = 90)\nplt.title(\"Number of Data Science Postings by City\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. *what average salaries are being offered in these cities?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating and plotting average salaries offered in different locations (average of the max)\ndf_sal_no_dup['Annual_Max_Salary'] = df_sal_no_dup['Annual_Max_Salary'].astype(int)\n# plot2 = df_sal_no_dup[['City','Annual_Max_Salary']].groupby('City').agg('mean').plot(kind = 'bar', color = 'gray', title = 'On Average, Maximum Salaries offered by different Cities')\nplot2 = df_sal_no_dup[['City','Annual_Max_Salary']].groupby('City').agg('mean').reset_index()\nsns.set_style(\"whitegrid\")\nsns.set()\nsns.barplot(x = plot2['City'], y = plot2['Annual_Max_Salary'], color='brown')\nplt.xticks(rotation = 90)\nplt.ylabel('Salary')\nplt.title(\"On Average, Maximum Salaries offered by different Cities\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. *Which companies are most actively looking for data scientists?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import chart_studio.plotly as py\n## Companies with highest n.o of postings\nplot3 = df_sal_no_dup[['ID','Company']].groupby(['Company']).agg('count').reset_index().rename(columns = {'ID': 'Count'})\nplot3 = plot3.loc[plot3['Count'] >= 10, :]\ndata_bar = [go.Bar(x = plot3['Company'] , y = plot3['Count'], name = 'company_count_barplot', marker = dict(color = '#109618'), width = 0.5)]\nlayout = go.Layout(title = 'Companies with 10 or more Data Science job postings', xaxis_tickangle = -90)\nfig = go.Figure(data = data_bar, layout = layout)\nfig.layout.template = 'plotly_white'\npyo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. *What are the salaries offered by these highly active companies?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Out of the 9 companies with highest n.o of job postings, which company is offering the highest salary on average\nplot4 = df_sal_no_dup.loc[df_sal_no_dup['Company'].isin(plot3['Company'].unique()), :]\nplot4 = plot4[['Company','Annual_Max_Salary']].groupby('Company').agg('mean').reset_index().rename(columns = {'Annual_Max_Salary' : 'Average Salary'})\ndata_bar2 = [go.Bar(x = plot4['Company'], y = plot4['Average Salary'], name = 'company_mean_sal_barplot', marker = dict(color = '#FF7F0E'), width = 0.5)]\nlayout = go.Layout(title = 'Average Salary offered by Companies with 10 or more Data Science job postings', xaxis_tickangle = -90)\nfig = go.Figure(data = data_bar2, layout = layout)\nfig.layout.template = 'plotly_white'\npyo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graphs help us better understand the data that we are dealing with. However, they also provide some insights.\nFor example, some job postings are outside the main cities. Hence, they need to be mapped to the nearest city.\nHence, we would need to create an additional column that maps these job postings to the nearest metropolitan city."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking n.o of unique cities\ndf_sal_no_dup['City'].unique()\n\n# df_sal_no_dup.loc[df_sal_no_dup['City'] == 'Davidson', :]\n\n# Grouping the cities\n# if New York, Brooklyn, Jersey City, Fort Lee then NYC\n# if Fort Mill, Huntersville, Davidson, Charlotte then Charlotte\n# if San Rafael, Oakland, Walnut Creek, San Francisco then San Francisco\n# Boston\n# if Burbank, Torrance, Woodland Hills, Cypress then Los Angeles\n# if Fort Meade, Hyattsville, Arlington, Greenbelt then Washington\n\ncity_mapping = {'New York': 'New York City', 'Brooklyn' : 'New York City', 'Jersey City' : 'New York City', 'Fort Lee' : 'New York City',\n                'Fort Mill':'Charlotte','Huntersville':'Charlotte','Davidson':'Charlotte',\n                'San Rafael':'San Francisco','Oakland':'San Francisco','Walnut Creek':'San Francisco',\n                'Burbank':'Los Angeles','Torrance':'Los Angeles','Woodland Hills':'Los Angeles', 'Cypress':'Los Angeles',\n                'Fort Meade':'Washington','Hyattsville':'Washington','Arlington':'Washington','Greenbelt':'Washington'\n        }\n# print(city_mapping)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-mapping to major cities\ndf_sal_no_dup['Major_City'] = df_sal_no_dup['City']\ndf_sal_no_dup = df_sal_no_dup.replace({'Major_City': city_mapping})\n# df_sal_no_dup.loc[df_sal_no_dup['Major_City'].isnull(),:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color =green> 5. Future Scope </font>"},{"metadata":{},"cell_type":"markdown","source":"1. Perform in-depth inferential analysis on data science job market statistics and make recommendations based on job location, type, salary and company.\n2. Create a predictive model that predicts whether a particular candidate will accept or reject the offer based on various features associated with a company's job posting.\n3. Rank different companies based on features provided by the applicant in order to optimise job application process and identify most suitable companies"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}